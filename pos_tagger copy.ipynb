{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "#importing required libraries \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from conllu import parse\n",
    "import numpy as np\n",
    "import random as rd \n",
    "from random import shuffle                                                      \n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'en_atis-ud-train.conllu'\n",
    "test_file = 'en_atis-ud-test.conllu'\n",
    "dev_file = 'en_atis-ud-dev.conllu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = {}\n",
    "raw_datasets['train'] = parse(open(train_file, 'r').read())\n",
    "raw_datasets['dev'] = parse(open(dev_file, 'r').read())\n",
    "raw_datasets['test'] = parse(open(test_file, 'r').read())\n",
    "datasets = {}\n",
    "datasets['test'] = []\n",
    "datasets['train'] = []\n",
    "datasets['dev'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "\n",
    "def get_device():\n",
    "    '''\n",
    "    This function returns the device on which the model will be trained\n",
    "    '''\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "\n",
    "#reading data from the file \n",
    "def read_file(file_name):\n",
    "    '''\n",
    "    This function reads the file and returns the data in the form of list of sentences.\n",
    "    files shoulf be in conllu format \n",
    "    conllu file is parsed using conllu library\n",
    "    args : file_name\n",
    "    returns : list of sentences\n",
    "    '''\n",
    "    \n",
    "    raw_data = parse(open(file_name).read())\n",
    "    return raw_data\n",
    "\n",
    "def create_datasets(file_path , word2idx , tag2idx , train=False):\n",
    "    '''\n",
    "    This function creates the dataset from the raw data\n",
    "    args : raw_data\n",
    "    returns : dataset where each element is a tuple of (sentence , tags)\n",
    "    '''\n",
    "    dataset = [[] , []]\n",
    "    if train: \n",
    "        raw_data = read_file(file_path)\n",
    "        words = []\n",
    "        tags = []\n",
    "        for sentence in raw_data:\n",
    "            sentence_words = []\n",
    "            sentence_tags = []\n",
    "\n",
    "            for word in sentence:\n",
    "                if word['form'] not in word2idx:\n",
    "                    word2idx.update({word['form']:len(word2idx)})\n",
    "                if word['upostag'] not in tag2idx:\n",
    "                    tag2idx.update({word['upostag']:len(tag2idx)})\n",
    "                sentence_tags.append(tag2idx[word['upostag']])\n",
    "                sentence_words.append(word['form'])\n",
    "            dataset[0].append(sentence_words)\n",
    "            dataset[1].append(sentence_tags)\n",
    "        return dataset \n",
    "    else:\n",
    "        raw_data= read_file (file_path)\n",
    "        for sentence in raw_data:\n",
    "            sentence_words = []\n",
    "            sentence_tags = []\n",
    "            for word in sentence:\n",
    "                new_word = word['form']\n",
    "                new_tag = word['upostag']\n",
    "                if new_word not in word2idx:\n",
    "                    new_word = '<UNK>'\n",
    "                sentence_words.append(new_word)\n",
    "                if new_tag not in tag2idx:\n",
    "                    tag2idx.update({new_tag:len(tag2idx)})\n",
    "                sentence_tags.append(tag2idx[new_tag])\n",
    "            dataset[1].append(sentence_tags)\n",
    "            dataset[0].append(sentence_words)\n",
    "        return dataset\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word2idx = {}\n",
    "word2idx = {'<UNK>': 0}\n",
    "tag2idx = {}\n",
    "train_loader = []\n",
    "dev_loader = []\n",
    "test_loader = []\n",
    "\n",
    "\n",
    "\n",
    "class POS_Dataset(Dataset):\n",
    "    '''\n",
    "    This class creates the dataset for the model\n",
    "    '''\n",
    "\n",
    "    def __init__(self ,words , tags , word2idx ):\n",
    "        super(POS_Dataset , self).__init__()\n",
    "        '''\n",
    "        This function initializes the dataset\n",
    "        args : data_dir , word2idx , tag2idx\n",
    "        '''\n",
    "        self.words = torch.tensor(words)\n",
    "        self.tags = torch.tensor(tags)\n",
    "        self.word2idx = word2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        This function returns the length of the dataset\n",
    "        '''\n",
    "        return len(self.words)\n",
    "    \n",
    "    def __getitem__(self , idx):\n",
    "        '''\n",
    "        This function returns the item at the given index\n",
    "        '''\n",
    "        return self.words[idx] , self.tags[idx]\n",
    "    \n",
    "\n",
    "datasets['train'] = create_datasets(train_file , word2idx , tag2idx , train=True)\n",
    "loader = DataLoader(datasets['train'] , batch_size=32 , shuffle=True)\n",
    "print(type(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class textDataset(torch.utils.data.Dataset):\n",
    "#     def __init__\n",
    "\n",
    "\n",
    "class POS_tagger(nn.Module):\n",
    "    def __init__(self, embedding_dim , hidden_dim ,hidden_dim2, vocab_size , tagset_size ):\n",
    "        super(POS_tagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim,hidden_dim2)\n",
    "        \n",
    "        self.hidden2tag2 = nn.Linear(hidden_dim2,tagset_size)\n",
    "       \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode1 = nn.Sequential(nn.Embedding(len(word2idx), 100), nn.LSTM(100, 100), nn.Linear(100, len(tag2idx)), nn.LogSoftmax(dim=1))\n",
    "model = POS_tagger(100, 100, len(word2idx), len(tag2idx))\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Train loss: 1.345.. Test loss: 0.904.. Test accuracy: 0.708\n",
      "Epoch 1/50 Train loss: 0.985.. Test loss: 0.807.. Test accuracy: 0.740\n",
      "Epoch 1/50 Train loss: 0.770.. Test loss: 0.770.. Test accuracy: 0.758\n",
      "Epoch 1/50 Train loss: 0.818.. Test loss: 0.713.. Test accuracy: 0.767\n",
      "Epoch 1/50 Train loss: 0.833.. Test loss: 0.749.. Test accuracy: 0.768\n",
      "Epoch 1/50 Train loss: 0.787.. Test loss: 0.714.. Test accuracy: 0.779\n",
      "Epoch 1/50 Train loss: 0.784.. Test loss: 0.716.. Test accuracy: 0.783\n",
      "Epoch 1/50 Train loss: 0.739.. Test loss: 0.654.. Test accuracy: 0.787\n",
      "Epoch 1/50 Train loss: 0.760.. Test loss: 0.685.. Test accuracy: 0.789\n",
      "Epoch 1/50 Train loss: 0.818.. Test loss: 0.701.. Test accuracy: 0.792\n",
      "Epoch 1/50 Train loss: 0.812.. Test loss: 0.641.. Test accuracy: 0.796\n",
      "Epoch 1/50 Train loss: 0.692.. Test loss: 0.632.. Test accuracy: 0.803\n",
      "Epoch 1/50 Train loss: 0.732.. Test loss: 0.606.. Test accuracy: 0.803\n",
      "Epoch 1/50 Train loss: 0.701.. Test loss: 0.635.. Test accuracy: 0.807\n",
      "Epoch 1/50 Train loss: 0.672.. Test loss: 0.631.. Test accuracy: 0.809\n",
      "Epoch 1/50 Train loss: 0.676.. Test loss: 0.595.. Test accuracy: 0.811\n",
      "Epoch 3/50 Train loss: 0.678.. Test loss: 0.659.. Test accuracy: 0.811\n",
      "Epoch 7/50 Train loss: 0.697.. Test loss: 0.641.. Test accuracy: 0.811\n",
      "Epoch 11/50 Train loss: 0.622.. Test loss: 0.622.. Test accuracy: 0.815\n",
      "Epoch 11/50 Train loss: 0.664.. Test loss: 0.609.. Test accuracy: 0.816\n",
      "Epoch 13/50 Train loss: 0.576.. Test loss: 0.602.. Test accuracy: 0.817\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m loss \u001b[39m=\u001b[39m criterion(tag_scores, tags)\n\u001b[1;32m     16\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 17\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     18\u001b[0m RUNNING_LOSS \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m STEPS \u001b[39m%\u001b[39m PRINT_EVERY \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchy/lib/python3.10/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchy/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchy/lib/python3.10/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    158\u001b[0m          grads,\n\u001b[1;32m    159\u001b[0m          exp_avgs,\n\u001b[1;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    162\u001b[0m          state_steps,\n\u001b[1;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/torchy/lib/python3.10/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m func(params,\n\u001b[1;32m    214\u001b[0m      grads,\n\u001b[1;32m    215\u001b[0m      exp_avgs,\n\u001b[1;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    218\u001b[0m      state_steps,\n\u001b[1;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchy/lib/python3.10/site-packages/torch/optim/adam.py:262\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    259\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[1;32m    261\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m    263\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    265\u001b[0m \u001b[39mif\u001b[39;00m capturable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "EPOCHS = 50 \n",
    "STEPS = 0 \n",
    "best_accuracy = 0\n",
    "RUNNING_LOSS = 0\n",
    "PRINT_EVERY = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    if epoch % 10 == 0:\n",
    "        optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * 0.5\n",
    "    for sentence, tags in train_loader:\n",
    "        STEPS += 1\n",
    "        optimizer.zero_grad()\n",
    "        tag_scores = model(sentence)\n",
    "        # print(sentence)\n",
    "        loss = criterion(tag_scores, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        RUNNING_LOSS += loss.item()\n",
    "        if STEPS % PRINT_EVERY == 0:\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            with torch.no_grad():\n",
    "                for sentence, tags in dev_loader:\n",
    "                    tag_scores = model(sentence)\n",
    "                    test_loss += criterion(tag_scores, tags)\n",
    "                    ps = torch.exp(tag_scores)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == tags.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                torch.save(model.state_dict(), 'pos_tagger3.pth')\n",
    "                print(f\"Epoch {epoch+1}/{EPOCHS} \"\n",
    "                  f\"Train loss: {RUNNING_LOSS/PRINT_EVERY:.3f}.. \"\n",
    "                  f\"Test loss: {test_loss/len(dev_loader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(dev_loader):.3f}\")\n",
    "            \n",
    "            model.train()\n",
    "            RUNNING_LOSS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.961\n"
     ]
    }
   ],
   "source": [
    "model = POS_tagger(100, 100, len(word2idx), len(tag2idx))\n",
    "model.load_state_dict(torch.load('pos_tagger2.pth'))\n",
    "model.eval()\n",
    "accuracy = 0\n",
    "\n",
    "for sentence, tags in test_loader:\n",
    "    tag_scores = model(sentence)\n",
    "    ps = torch.exp(tag_scores)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    equals = top_class == tags.view(*top_class.shape)\n",
    "    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "print(f\"Test accuracy: {accuracy/len(test_loader):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRON\n",
      "NOUN\n",
      "DET\n",
      "NOUN\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "input = 'mary had a little lamb'\n",
    "input = input.split()\n",
    "input_new = []\n",
    "for word in input:\n",
    "    if word not in word2idx:\n",
    "        input_new.append(word2idx['<UNK>'])\n",
    "    else :\n",
    "        input_new.append(word2idx[word])\n",
    "\n",
    "input_new = torch.tensor(input_new, dtype=torch.long)\n",
    "# print(input_new)\n",
    "tag_scores = model(input_new)\n",
    "# print(tag_scores)\n",
    "ps = torch.exp(tag_scores)\n",
    "top_p, top_class = ps.topk(1, dim=1)\n",
    "# print(top_p)\n",
    "idxtag = {v: k for k, v in tag2idx.items()}\n",
    "for i in range(len(top_class)):\n",
    "    print(idxtag[top_class[i][0].item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
